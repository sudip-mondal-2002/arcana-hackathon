# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j6Yh0pbh1iXNOIgTxiU8zBKFWvxJYuUT
"""

# Commented out IPython magic to ensure Python compatibility.
import sys
from bs4 import BeautifulSoup
from datetime import datetime
import requests
import pandas as pd
import numpy as np
import re
from pathlib import Path
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
import gensim
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from pandas_datareader import data as pdr
from datetime import timedelta

# %matplotlib inline

dir="./dataset/2022_1.json"
loughram_dir="./dataset/master_dict.csv"

df = pd.read_json(dir, orient ='index', convert_axes=False)

np_array=df.to_numpy()
np_array=np_array.transpose()
df=pd.DataFrame(np_array)

df

df.columns=["ticker", "quarter", "year", "date", "content"]

#load the spacy model
spacy_model = 'en_core_web_sm'

nlp=spacy.load(spacy_model)

#preprocessing
CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')
def cleanhtml(raw_html):
  cleantext = re.sub(CLEANR, '', raw_html)
  cleantext=re.sub("\n|\r", ".", cleantext)
  cleantext = re.sub("\s\s+|\t", " ", cleantext)
  cleantext=re.sub("--", " ", cleantext)
  return cleantext

df["content"]=df["content"].apply(cleanhtml)

#pd.set_option('max_colwidth', -1)
df["content"]

additional_stop_words = ['hi', 'earning', 'conference', 'speaker', 'analyst', 'operator', 'welcome', \
                         'think', 'cost', 'result', 'primarily', 'overall', 'line', 'general', \
                          'thank', 'see', 'alphabet', 'google', 'facebook', 'amazon', 'microsoft',\
                        'business', 'customer', 'revenue', 'question', 'lady', 'gentleman', \
                        'continue', 'continuing', 'continued', 'focus', 'participant', 'see', 'seeing', \
                        'user', 'work', 'lot', 'day',  'like', 'looking', 'look', 'come', 'yes', 'include', \
                        'investor', 'director', 'expense', 'manager', 'founder', 'chairman', \
                         'chief', 'operating', 'officer', 'executive', 'financial', 'senior', 'vice', 'president', \
                        'opportunity', 'go', 'expect', 'increase', 'quarter', 'stand', 'instructions', \
                        'obviously', 'thing', 'important', 'help', 'bring', 'mention', 'yeah', 'get', 'proceed', \
                        'currency', 'example', 'believe'] 

for stopword in additional_stop_words:
    nlp.vocab[stopword].is_stop = True

def get_cleaned_word_list(sentence):
    """
    Returns the list of words after removing non-alphanumeric words, stop words, prepositions and names of person
    
    Parameters
    ----------
    df_row : dataframe row
       The row of the dataframe with information of the local html
    """

    words = []

    doc = nlp(sentence)
    with doc.retokenize() as retokenizer:
        for ent in doc.ents:
            # print(ent.text, ent.label_)
            retokenizer.merge(doc[ent.start:ent.end], attrs={"LEMMA": ent.text})
    #print('-------------------')
    for word in doc:
        # print(word, word.lemma_, word.ent_type_)
        if word.is_alpha and word.is_ascii and not word.is_stop and \
            word.ent_type_ not in ['PERSON','DATE', 'TIME', 'ORDINAL', 'CARDINAL'] and \
            word.text.lower() not in additional_stop_words and \
            word.lemma_.lower() not in additional_stop_words:
                #print(word)
                words.append(word.lemma_.lower())
    return words

df["cleaned_content"]=df["content"].apply(get_cleaned_word_list)

df["cleaned_content"][0]

# new bigram based stop words which can go
# additional_stop_words=['gross_margin', 'come_line', 'constant_currency', 'operatorthank_proceed', 'long_term', \
#                       'grow_constant', 'year_year' , 'growth', 'strong', 'great', 'maybe']
# def get_lemmatized_words(content):
#     """
#     Get the lemmatized words after removing the new bigram based stopd words
#     Parameters
#     ----------
#     df_row : dataframe row
#        The row of the dataframe with information of the local html
#     """
#     words = []
#     doc=df["cleaned_content"]
#     for word in doc:
#         if word not in additional_stop_words:
#             words.append(word)
#     return words

df

tfidf_vectorizer = TfidfVectorizer(smooth_idf=True,use_idf=True, min_df=0.025)

#predict sentiment scores

sentiments = ['negative', 'positive', 'uncertainty', 'litigious', 'constraining']

sentiment_df = pd.read_csv(loughram_dir)
sentiment_df.columns = [column.lower() for column in sentiment_df.columns] # Lowercase the columns for ease of use

sentiment_df

sentiment_df

sentiment_df = sentiment_df[['negative', 'positive', 'uncertainty', 'litigious', 'constraining','word']]
sentiment_df[sentiments] = sentiment_df[sentiments].astype(bool)
print(f'before any sentiment_df.shape {sentiment_df.shape}')
sentiment_df = sentiment_df[(sentiment_df[sentiments]).any(1)]
print(f'after any sentiment_df.shape {sentiment_df.shape}')

word_list = nlp(" ".join(sentiment_df['word'].str.lower()))
word_lemmas = []
for word in word_list:
    # print(word.text, word.lemma_)
    word_lemmas.append(word.lemma_)

sentiment_df

sentiment_df.insert(loc=6, column='lemma', value=word_lemmas)

# Apply the same preprocessing to these words as the 10-k words
sentiment_df = sentiment_df.drop_duplicates('lemma')
print(f'after drop_duplicates sentiment_df.shape {sentiment_df.shape}')
print()
print(f'sentiment_df[sentiments].sum()-->\n{sentiment_df[sentiments].sum()}')
print('shape is sentiment * words i.e. rows will have sentiment classification and columns are list of words with those sentiments')

df = df.reindex(columns= df.columns.to_list() + sentiments)

sentiment_vectorizer = CountVectorizer()

def get_sentiment_info(df_row, sentiment_vectorizer):
    """
    Get count of the words of a particular sentiment in the transcript
    Parameters
    ----------
    df_row : dataframe row
       The row of the dataframe with words_ngrams
    sentiment_vectorizer: sentiment_vectorizer instance
        To count the instance of the word sentiment
    """
    vector = sentiment_vectorizer.transform([" ".join(df_row['cleaned_content'])])
    return np.sum(vector.toarray())

for sentiment in sentiments:
    sentiment_words = sentiment_df.loc[sentiment_df[sentiment],'lemma']
    sentiment_vectorizer.fit(sentiment_words)
    df[sentiment] = df.apply(get_sentiment_info, args=(sentiment_vectorizer, ), axis=1)

df

df[sentiments]
